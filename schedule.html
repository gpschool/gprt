---
layout: page
title: Schedule
---


<h1>GPRT Schedule</h1>

<p>
<!--webbot bot="Navigation" S-Orientation="horizontal" S-Rendering="graphics" S-Type="siblings" B-Include-Home="FALSE" B-Include-Up="TRUE" startspan --><nobr>[&nbsp;<a href="index.html" target="">Up</a>&nbsp;]</nobr> <nobr>[&nbsp;GPRT&nbsp;Schedule&nbsp;]</nobr> <nobr>[&nbsp;<a href="programme.html" target="">GPRT&nbsp;Programme</a>&nbsp;]</nobr><!--webbot bot="Navigation" i-checksum="1334" endspan --></p>

<p>&nbsp;</p>

<h2>Thursday 9th June</h2>
<table width="100%">
  <tr>
    <td width="10%" ><a name="welcome"></a>09:00 - 9:15</td>
    <td width="90%" ><a href="slides/welcome.ppt"> Welcome</a></td>
  </tr>
  <tr>
    <td width="10%" ><a name="ohagan"></a>09:15 - 10:15</td>
    <td width="90%" ><a href="slides/tonyohagan.ppt"> Gaussian Processes I Have Known</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" ><a href="http://www.shef.ac.uk/~st1ao/">Tony 
    O'Hagan</a>, <i>Department of Probability and Statistics, University of 
    Sheffield, U.K.</i></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >&nbsp;</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="williams"></a>10:15 - 11:15 </td>
    <td width="90%" ><a href="slides/chriswilliams.pdf">Approximate Methods for GP Regression: A Survey and an Empirical 
    Comparison</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.dai.ed.ac.uk/homes/ckiw/">Chris Williams</a>, <i>School 
    of Informatics, University of Edinburgh</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">&nbsp;</td>
  </tr>
</table>
<table  width="100%">
  <tr>
    <td width="10%" >11:15 - 11:45</td>
    <td width="90%" ><b>Coffee Break</b></td>
  </tr>
</table>
<table  width="100%">
  <tr>
    <td width="10%" ><a name="yu"></a>11:45 - 12:45</td>
    <td width="90%" >
    <a href="slides/kaiyu.pdf">Nonparametric Bayesian Models in Machine Learning</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.dbs.informatik.uni-muenchen.de/~yu_k/">Kai Yu</a>, <i>
    Siemens AG, Germany</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">&nbsp;</td>
  </tr>
</table>
<table  width="100%">
 <tr>
    <td width="10%" >12:45 - 13:45</td>
    <td width="90%" ><b>Lunch</b></td>
  </tr>
 </table> 
<table  width="100%">
  <tr>
    <td width="10%" ><a name="ghahramani"></a>13:45 - 14:00</td>
    <td width="90%" >
    <p wrap><b>Some thoughts on Gaussian Processes</b></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.gatsby.ucl.ac.uk/~zoubin/">Zoubin Ghahramani</a>, <i>The 
    Gatsby Institute, University College London, U.K.</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">
&nbsp;</td>
  </tr>
</table>
<table  width="100%">
  <tr>
    <td width="10%" ><a name="snelson"></a>14:00 - 14:45</td>
    <td width="90%" >
    <p wrap><a href="slides/edsnelson.pdf">Sparse Parametric Gaussian Processes</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.gatsby.ucl.ac.uk/~snelson/">Ed Snelson</a>, <i>The 
    Gatsby Institute, University College London, U.K.</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">
&nbsp;</td>
  </tr>
</table>
<table  width="100%">
  <tr>
    <td width="10%" ><a name="niranjan"></a>14:45 - 15:45</td>
    <td width="90%" ><b>Towards bridging the gap between transcriptome and proteome
measurement uncertainties with Gaussian processes</b></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" ><a href="http://www.dcs.shef.ac.uk/~niranjan">Niranjan</a>,
    <i>Department of Computer Science, University of Sheffield, U.K.</i></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >&nbsp;</td>
  </tr>
  <tr>
    <td width="10%" >15:45 - 16:15</td>
    <td width="90%" ><b>Coffee Break</b></td>
  </tr>
</table>
<table  width="100%">
  <tr>
    <td width="10%" ><a name="rasmussen"></a>16:15 - 17:15</td>
    <td width="90%" ><b>Assessing Approximations for Gaussian Process 
    Classification</b></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" ><a href="http://www.kyb.tuebingen.mpg.de/~carl">Carl 
    Rasmussen</a>,
    <i>Max-Planck Institute, Tuebingen, Germany</i></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" ><i>Based on joint work with Malte Kuss. <br>
    </i><br>
    The computations required for Gaussian Process Classification are 
    analytically intractable. Several approximation schemes have been proposed 
    recently, but at present it is less than clear how well each of these 
    perform. I compare the Laplace approximation to the Expectation Propagation 
    (EP) algorithm on the evaluation of two key quanteties: the marginal 
    likelihood and the predictive probabilities. I also compare to ground truth, 
    tortuously obtained by Annealed Importance Sampling (MCMC). <br>
    <br>
&nbsp;</td>
  </tr>
</table>
<h2>Friday 10th June</h2>
<table width="100%">
  <tr>
    <td width="10%" ><a name="lawrence"></a>9:00 - 9:30</td>
    <td width="90%" >
    <a href="slides/neillawrence.pdf">KL Corrected Variational Inference for Gaussian Processes</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
<p>
    <a href="http://www.dcs.shef.ac.uk/~neil">Neil Lawrence</a>, <i>Department 
    of Computer Science, University of Sheffield, U.K.</i></p>
    </td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
&nbsp;</td>
  </tr>
  <tr>
    <td width="10%" ><a name="opper"></a>9:30 - 10:00</td>
    <td width="90%" >
    <a href="slides/manfredopper.pdf">Resampling PCA & GP Inference</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
<a href="http://www.ecs.soton.ac.uk/people/mo">Manfred Opper</a>, <i>School of 
Electronics and Computer Science, University of Southampton, U.K.</i></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
&nbsp;</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="winther"></a>10:00 - 11:00</td>
    <td width="90%" ><a href="slides/olewinter1.ps">Joint Gaussian Process-Density Mixtures</b></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
<p><a href="http://eivind.imm.dtu.dk/staff/winther/">Ole Winther</a>, <i>
Technical University of Denmark, Denmark</i></p>
    </td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">
Gaussian Processes (GPs) provide a natural framework for Bayesian kernel 
methods. This talk will be about some work in progress on combining GPs with 
density estimation in a mixture model. The motivations are: using kernels tuned 
individually to each mixture component gives a more flexible input-output model, 
unlabelled data can be used in a semi-supervised setting and the computational 
complexity can be reduced because only examples belonging to the same mixture 
component need to be included in the kernel matrix for that mixture component. A 
variational Bayes treatment of the joint estimation problem shows how a low 
complexity solution can be obtained. A more precise approximation to the 
inference problem of the expectation consistent/propagation type is also 
possible.</td>
  </tr>
  <tr>
  <tr>
    <td width="10%" >10:00 - 11:00</td>
    <td width="90%" ><a href="slides/olewinter2.pdf">Expectation Consistent Approximate Inference</b></td>
  </tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
<p><a href="http://eivind.imm.dtu.dk/staff/winther/">Ole Winther</a>, <i>
Technical University of Denmark, Denmark</i></p>
    </td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" >11:00 - 11:30</td>
    <td width="90%" ><b>Coffee Break</b></td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="schwaighofer"></a>11:30 - 12:30</td>
    <td width="90%" >
    <a href="slides/antonschwaighofer.pdf">Requirements for GPC in the Real World</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.igi.tugraz.at/aschwaig/">Anton Schwaighofer</a>, <i>GMD 
    First</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">
&nbsp;</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" >12:30 - 14:00</td>
    <td width="90%" ><b>Lunch</b></td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="csato"></a>14:00 - 15:00</td>
    <td width="90%" ><a href="slides/lehelcsato.pdf">Sparsity in Gaussian Processes: Questions</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.kyb.mpg.de/~csatol">Lehel Csato</a>, <i>Max-Planck 
    Institute, Tuebingen, Germany</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">&nbsp;</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="dodd"></a>15:00 - 16:00</td>
    <td width="90%" >
    <a href="slides/tonydodd.pdf">Issues and Challenges in On-Line Gaussian Process Estimation</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.acse.shef.ac.uk/~dodd/">Tony Dodd</a>, <i>Automatic 
    Control and Systems Engineering, University of Sheffield, U.K.</i></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    Gaussian processes (GPs) have been successfully applied to a number of 
    well-known problems in machine learning, signal processing and function 
    approximation. Much of the interest in GPs arises from the multiple 
    interpretations possible: statistical, (Bayesian) probabilistic and 
    reproducing kernel Hilbert spaces (RKHS). Previous research has focused on 
    batch learning for GPs where it is assumed all the data is available. 
    Recently there has been interest in on-line or sequential learning of GPs. 
    This has applications to incremental solutions for large data set problems, 
    on-line learning and adaptive non-stationary modelling. Issues and 
    challenges relating to the use of on-line Gaussian process models in machine 
    learning will be presented. These will include the need to provably 
    guarantee convergence, convergence rates and how to compute the models 
    efficiently. Preliminary results on some of these will be discussed.</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" >16:00 - 16:30</td>
    <td width="90%" ><b>Tea</b></td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="candela"></a>16:30 - 17:30</td>
    <td width="90%" >
    <a href="slides/joaquin.pdf">Some concerns about computationally efficient 
    approximations to GPs</a></td>
  </tr>
  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.kyb.tuebingen.mpg.de/~jqc">
    Joaquin Quinonero Candela</a>, <i>Max-Planck Institute, Tuebingen, Germany</i></td>
  </tr>
  <tr>
    <td width="10%">&nbsp;</td>
    <td width="90%">
&nbsp;</td>
  </tr>
</table>
